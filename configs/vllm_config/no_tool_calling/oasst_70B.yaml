model: "TheBloke/Llama2-70B-OASST-SFT-v10-GPTQ"
download_dir: /srv/llm-weights
served-model-name: "default"
host: "localhost"
port: 8000

enable-prefix-caching: true
quantization: bitsandbytes

# Max size for llama-2 based models are 4096, can only be increased using rope-scaling
rope-scaling: '{"type":"linear","factor":2.0}'

max-model-len: 8192
max_num_batched_tokens: 8192

tensor-parallel-size: 1

# Llama based models need chat_template to defined manually
chat_template: |
  {% set loop_messages = messages %}
  {% for message in loop_messages %}
  {% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}
  {% if loop.index0 == 0 %}
  {% set content = bos_token + content %}
  {% endif %}
  {{ content }}
  {% endfor %}
  {% if add_generation_prompt %}
  {{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}
  {% endif %}
